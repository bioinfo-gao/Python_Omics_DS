{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e37a7a7",
   "metadata": {},
   "source": [
    "相似度数据集 AFQMC 作为语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d10a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': '蚂蚁借呗等额还款可以换成先息后本吗', 'sentence2': '借呗有先息到期还本吗', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class AFQMC(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = AFQMC('data/afqmc_public/train.json')\n",
    "valid_data = AFQMC('data/afqmc_public/dev.json')\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733ffc0",
   "metadata": {},
   "source": [
    "如果数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5ed8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': '蚂蚁借呗等额还款可以换成先息后本吗', 'sentence2': '借呗有先息到期还本吗', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "import json\n",
    "\n",
    "class IterableAFQMC(IterableDataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.data_file, 'rt') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                yield sample\n",
    "\n",
    "train_data = IterableAFQMC('data/afqmc_public/train.json')\n",
    "\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89059",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "接下来就需要通过 DataLoader 库按批 (batch) 加载数据，并且将样本转换成模型可以接受的输入格式。对于 NLP 任务，这个环节就是将每个 batch 中的文本按照预训练模型的格式进行编码（包括 Padding、截断等操作）。\n",
    "\n",
    "我们通过手工编写 DataLoader 的批处理函数 collate_fn 来实现。首先加载分词器，然后对每个 batch 中的所有句子对进行编码，同时把标签转换为张量格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6f2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 30]), 'token_type_ids': torch.Size([4, 30]), 'attention_mask': torch.Size([4, 30])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 6010, 6009,  955, 1446, 5023, 7583, 6820, 3621, 1377,  809, 2940,\n",
      "         2768, 1044, 2622, 1400, 3315, 1408,  102,  955, 1446, 3300, 1044, 2622,\n",
      "         1168, 3309, 6820, 3315, 1408,  102],\n",
      "        [ 101, 6010, 6009, 5709, 1446, 6432, 2769, 6824, 5276,  671, 3613,  102,\n",
      "         6010, 6009, 5709, 1446, 6824, 5276, 6121,  711, 3221,  784,  720,  102,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2376, 2769, 4692,  671,  678, 3315, 3299, 5709, 1446, 6572, 1296,\n",
      "         3300, 3766, 3300, 5310, 3926,  102,  678, 3299, 5709, 1446, 6572, 1296,\n",
      "          102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6010, 6009,  955, 1446, 1914, 7270, 3198, 7313, 5341, 1394, 6397,\n",
      "          844,  671, 3613,  102,  955, 1446, 2533, 6397,  844, 1914,  719,  102,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0]])}\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "#train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf99b7",
   "metadata": {},
   "source": [
    "这个错误是因为您的 train_data 是一个 IterableDataset，而 IterableDataset 不能直接使用 shuffle=True 参数。我来帮您修改代码并提供解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051ed1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 67]), 'token_type_ids': torch.Size([4, 67]), 'attention_mask': torch.Size([4, 67])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 2166,  802, 3221, 4500, 5709, 1446, 6820, 3221,  865, 7583,  102,\n",
      "         5709, 1446,  679, 1377,  809, 4500, 2166,  802,  802, 3621, 1408,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5709, 1446, 6818, 3309, 3833, 1220,  102, 5709, 1446, 3300, 3173,\n",
      "         3833, 1220, 1408,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5709, 1446, 1377,  809, 1762, 3118,  802, 2140, 7027, 4157, 1912,\n",
      "         1297, 1658,  102, 1912, 1297, 1555, 2157, 1377,  809,  886, 4500, 5709,\n",
      "         1446, 1658,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2769, 1762,  671,  702, 3118,  802, 2140, 6572, 1384, 2458, 6858,\n",
      "          749, 5709, 1446, 8024, 4385, 1762, 1068, 7308,  749, 8024,  711,  784,\n",
      "          720, 1762, 1369,  671,  702, 3118,  802, 2140, 6572, 1384, 1316,  679,\n",
      "         5543, 2458, 6858,  749,  102, 2769, 1399,  678, 1372, 3300,  671,  702,\n",
      "         3118,  802, 2140, 8024, 5709, 1446, 2582,  720, 3227, 4850, 2769, 3300,\n",
      "         1369,  671,  702, 3118,  802, 2140,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "# 方法1：使用缓冲池实现IterableDataset的shuffle功能\n",
    "class ShuffledIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset, buffer_size=1000):\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for sample in self.dataset:\n",
    "            buffer.append(sample)\n",
    "            if len(buffer) >= self.buffer_size:\n",
    "                # 从缓冲区随机选择一个样本\n",
    "                idx = random.randint(0, len(buffer) - 1)\n",
    "                yield buffer.pop(idx)\n",
    "        \n",
    "        # 处理缓冲区剩余样本\n",
    "        while buffer:\n",
    "            idx = random.randint(0, len(buffer) - 1)\n",
    "            yield buffer.pop(idx)\n",
    "\n",
    "# 包装您的train_data\n",
    "shuffled_train_data = ShuffledIterableDataset(train_data, buffer_size=1000)\n",
    "\n",
    "# 使用shuffle=False（因为已经在ShuffledIterableDataset中实现了shuffle）\n",
    "train_dataloader = DataLoader(shuffled_train_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160b1df",
   "metadata": {},
   "source": [
    "替代方案（更简单的方法）\n",
    "如果您的数据集不是特别大，建议直接转换为Map-style数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ccf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 39]), 'token_type_ids': torch.Size([4, 39]), 'attention_mask': torch.Size([4, 39])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 5709, 1446, 4385, 1762, 1108, 5310, 5543,  886, 4500, 1408,  102,\n",
      "         2769, 4385, 5709, 1446, 6820, 5543, 4500,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101,  955, 1446, 1377,  809,  679, 4500, 1146, 3309, 6820, 3621, 1408,\n",
      "          102,  955, 1446, 1146, 3309, 2797, 3322, 1377,  809,  671, 3613, 2595,\n",
      "          802, 1408,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 1963, 3362, 6010, 6009,  955, 1446, 1728,  711, 2990, 1184, 6820,\n",
      "         3621, 6158, 4881, 3632,  886, 4500, 8024,  809, 1400, 6820, 5543,  886,\n",
      "         4500, 1408,  102, 6010, 6009,  955, 1446, 6874, 3309,  749, 6820, 5543,\n",
      "         4500, 1408,  102],\n",
      "        [ 101, 5709, 1446, 3118,  802, 2458, 6858,  102, 2769, 2347, 2458, 6858,\n",
      "         5709, 1446, 3119, 3621,  852, 3221, 2145, 2787,  679, 5543, 4500, 5709,\n",
      "         1446, 3118,  802,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "# 将IterableDataset转换为普通的list（Map-style数据集）\n",
    "train_data_list = list(train_data)\n",
    "\n",
    "# 现在可以使用shuffle=True了\n",
    "train_dataloader = DataLoader(train_data_list, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3da4ed",
   "metadata": {},
   "source": [
    "主要修改内容\n",
    "修改1：添加了ShuffledIterableDataset类\n",
    "功能：实现了缓冲池shuffle算法，专门为IterableDataset设计\n",
    "\n",
    "原理：维护一个固定大小的缓冲区，从中随机选择样本输出\n",
    "\n",
    "参数：buffer_size控制shuffle的效果（越大越随机）\n",
    "\n",
    "修改2：包装原始数据集\n",
    "将原始的 train_data 包装在 ShuffledIterableDataset 中\n",
    "\n",
    "在 DataLoader 中使用 shuffle=False（因为shuffle已经在包装器中实现）\n",
    "\n",
    "修改3：添加了必要的import\n",
    "添加了 import random\n",
    "\n",
    "添加了 from torch.utils.data import IterableDataset\n",
    "\n",
    "💡 选择建议\n",
    "如果数据集很大（无法全部加载到内存）：使用第一种方法（ShuffledIterableDataset）\n",
    "\n",
    "如果数据集适中：使用第二种方法（转换为list），这样更简单直接\n",
    "\n",
    "如果使用Hugging Face datasets：检查是否可以使用 .to_list() 方法转换"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
