{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e37a7a7",
   "metadata": {},
   "source": [
    "ç›¸ä¼¼åº¦æ•°æ®é›† AFQMC ä½œä¸ºè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d10a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'èš‚èšå€Ÿå‘—ç­‰é¢è¿˜æ¬¾å¯ä»¥æ¢æˆå…ˆæ¯åæœ¬å—', 'sentence2': 'å€Ÿå‘—æœ‰å…ˆæ¯åˆ°æœŸè¿˜æœ¬å—', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class AFQMC(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_data = AFQMC('data/afqmc_public/train.json')\n",
    "valid_data = AFQMC('data/afqmc_public/dev.json')\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733ffc0",
   "metadata": {},
   "source": [
    "å¦‚æœæ•°æ®é›†éå¸¸å·¨å¤§ï¼Œéš¾ä»¥ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»§æ‰¿ IterableDataset ç±»æ„å»ºè¿­ä»£å‹æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5ed8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'èš‚èšå€Ÿå‘—ç­‰é¢è¿˜æ¬¾å¯ä»¥æ¢æˆå…ˆæ¯åæœ¬å—', 'sentence2': 'å€Ÿå‘—æœ‰å…ˆæ¯åˆ°æœŸè¿˜æœ¬å—', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "import json\n",
    "\n",
    "class IterableAFQMC(IterableDataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.data_file, 'rt') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line.strip())\n",
    "                yield sample\n",
    "\n",
    "train_data = IterableAFQMC('data/afqmc_public/train.json')\n",
    "\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e89059",
   "metadata": {},
   "source": [
    "DataLoader\n",
    "æ¥ä¸‹æ¥å°±éœ€è¦é€šè¿‡ DataLoader åº“æŒ‰æ‰¹ (batch) åŠ è½½æ•°æ®ï¼Œå¹¶ä¸”å°†æ ·æœ¬è½¬æ¢æˆæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼ã€‚å¯¹äº NLP ä»»åŠ¡ï¼Œè¿™ä¸ªç¯èŠ‚å°±æ˜¯å°†æ¯ä¸ª batch ä¸­çš„æ–‡æœ¬æŒ‰ç…§é¢„è®­ç»ƒæ¨¡å‹çš„æ ¼å¼è¿›è¡Œç¼–ç ï¼ˆåŒ…æ‹¬ Paddingã€æˆªæ–­ç­‰æ“ä½œï¼‰ã€‚\n",
    "\n",
    "æˆ‘ä»¬é€šè¿‡æ‰‹å·¥ç¼–å†™ DataLoader çš„æ‰¹å¤„ç†å‡½æ•° collate_fn æ¥å®ç°ã€‚é¦–å…ˆåŠ è½½åˆ†è¯å™¨ï¼Œç„¶åå¯¹æ¯ä¸ª batch ä¸­çš„æ‰€æœ‰å¥å­å¯¹è¿›è¡Œç¼–ç ï¼ŒåŒæ—¶æŠŠæ ‡ç­¾è½¬æ¢ä¸ºå¼ é‡æ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6f2a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 30]), 'token_type_ids': torch.Size([4, 30]), 'attention_mask': torch.Size([4, 30])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 6010, 6009,  955, 1446, 5023, 7583, 6820, 3621, 1377,  809, 2940,\n",
      "         2768, 1044, 2622, 1400, 3315, 1408,  102,  955, 1446, 3300, 1044, 2622,\n",
      "         1168, 3309, 6820, 3315, 1408,  102],\n",
      "        [ 101, 6010, 6009, 5709, 1446, 6432, 2769, 6824, 5276,  671, 3613,  102,\n",
      "         6010, 6009, 5709, 1446, 6824, 5276, 6121,  711, 3221,  784,  720,  102,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2376, 2769, 4692,  671,  678, 3315, 3299, 5709, 1446, 6572, 1296,\n",
      "         3300, 3766, 3300, 5310, 3926,  102,  678, 3299, 5709, 1446, 6572, 1296,\n",
      "          102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6010, 6009,  955, 1446, 1914, 7270, 3198, 7313, 5341, 1394, 6397,\n",
      "          844,  671, 3613,  102,  955, 1446, 2533, 6397,  844, 1914,  719,  102,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0]])}\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "#train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf99b7",
   "metadata": {},
   "source": [
    "è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºæ‚¨çš„ train_data æ˜¯ä¸€ä¸ª IterableDatasetï¼Œè€Œ IterableDataset ä¸èƒ½ç›´æ¥ä½¿ç”¨ shuffle=True å‚æ•°ã€‚æˆ‘æ¥å¸®æ‚¨ä¿®æ”¹ä»£ç å¹¶æä¾›è§£å†³æ–¹æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051ed1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 67]), 'token_type_ids': torch.Size([4, 67]), 'attention_mask': torch.Size([4, 67])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 2166,  802, 3221, 4500, 5709, 1446, 6820, 3221,  865, 7583,  102,\n",
      "         5709, 1446,  679, 1377,  809, 4500, 2166,  802,  802, 3621, 1408,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5709, 1446, 6818, 3309, 3833, 1220,  102, 5709, 1446, 3300, 3173,\n",
      "         3833, 1220, 1408,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5709, 1446, 1377,  809, 1762, 3118,  802, 2140, 7027, 4157, 1912,\n",
      "         1297, 1658,  102, 1912, 1297, 1555, 2157, 1377,  809,  886, 4500, 5709,\n",
      "         1446, 1658,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2769, 1762,  671,  702, 3118,  802, 2140, 6572, 1384, 2458, 6858,\n",
      "          749, 5709, 1446, 8024, 4385, 1762, 1068, 7308,  749, 8024,  711,  784,\n",
      "          720, 1762, 1369,  671,  702, 3118,  802, 2140, 6572, 1384, 1316,  679,\n",
      "         5543, 2458, 6858,  749,  102, 2769, 1399,  678, 1372, 3300,  671,  702,\n",
      "         3118,  802, 2140, 8024, 5709, 1446, 2582,  720, 3227, 4850, 2769, 3300,\n",
      "         1369,  671,  702, 3118,  802, 2140,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨ç¼“å†²æ± å®ç°IterableDatasetçš„shuffleåŠŸèƒ½\n",
    "class ShuffledIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset, buffer_size=1000):\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        for sample in self.dataset:\n",
    "            buffer.append(sample)\n",
    "            if len(buffer) >= self.buffer_size:\n",
    "                # ä»ç¼“å†²åŒºéšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬\n",
    "                idx = random.randint(0, len(buffer) - 1)\n",
    "                yield buffer.pop(idx)\n",
    "        \n",
    "        # å¤„ç†ç¼“å†²åŒºå‰©ä½™æ ·æœ¬\n",
    "        while buffer:\n",
    "            idx = random.randint(0, len(buffer) - 1)\n",
    "            yield buffer.pop(idx)\n",
    "\n",
    "# åŒ…è£…æ‚¨çš„train_data\n",
    "shuffled_train_data = ShuffledIterableDataset(train_data, buffer_size=1000)\n",
    "\n",
    "# ä½¿ç”¨shuffle=Falseï¼ˆå› ä¸ºå·²ç»åœ¨ShuffledIterableDatasetä¸­å®ç°äº†shuffleï¼‰\n",
    "train_dataloader = DataLoader(shuffled_train_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160b1df",
   "metadata": {},
   "source": [
    "æ›¿ä»£æ–¹æ¡ˆï¼ˆæ›´ç®€å•çš„æ–¹æ³•ï¼‰\n",
    "å¦‚æœæ‚¨çš„æ•°æ®é›†ä¸æ˜¯ç‰¹åˆ«å¤§ï¼Œå»ºè®®ç›´æ¥è½¬æ¢ä¸ºMap-styleæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ccf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 39]), 'token_type_ids': torch.Size([4, 39]), 'attention_mask': torch.Size([4, 39])}\n",
      "batch_y shape: torch.Size([4])\n",
      "{'input_ids': tensor([[ 101, 5709, 1446, 4385, 1762, 1108, 5310, 5543,  886, 4500, 1408,  102,\n",
      "         2769, 4385, 5709, 1446, 6820, 5543, 4500,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101,  955, 1446, 1377,  809,  679, 4500, 1146, 3309, 6820, 3621, 1408,\n",
      "          102,  955, 1446, 1146, 3309, 2797, 3322, 1377,  809,  671, 3613, 2595,\n",
      "          802, 1408,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 1963, 3362, 6010, 6009,  955, 1446, 1728,  711, 2990, 1184, 6820,\n",
      "         3621, 6158, 4881, 3632,  886, 4500, 8024,  809, 1400, 6820, 5543,  886,\n",
      "         4500, 1408,  102, 6010, 6009,  955, 1446, 6874, 3309,  749, 6820, 5543,\n",
      "         4500, 1408,  102],\n",
      "        [ 101, 5709, 1446, 3118,  802, 2458, 6858,  102, 2769, 2347, 2458, 6858,\n",
      "         5709, 1446, 3119, 3621,  852, 3221, 2145, 2787,  679, 5543, 4500, 5709,\n",
      "         1446, 3118,  802,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['sentence1'])\n",
    "        batch_sentence_2.append(sample['sentence2'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    X = tokenizer(\n",
    "        batch_sentence_1, \n",
    "        batch_sentence_2, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y\n",
    "\n",
    "# å°†IterableDatasetè½¬æ¢ä¸ºæ™®é€šçš„listï¼ˆMap-styleæ•°æ®é›†ï¼‰\n",
    "train_data_list = list(train_data)\n",
    "\n",
    "# ç°åœ¨å¯ä»¥ä½¿ç”¨shuffle=Trueäº†\n",
    "train_dataloader = DataLoader(train_data_list, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3da4ed",
   "metadata": {},
   "source": [
    "ä¸»è¦ä¿®æ”¹å†…å®¹\n",
    "ä¿®æ”¹1ï¼šæ·»åŠ äº†ShuffledIterableDatasetç±»\n",
    "åŠŸèƒ½ï¼šå®ç°äº†ç¼“å†²æ± shuffleç®—æ³•ï¼Œä¸“é—¨ä¸ºIterableDatasetè®¾è®¡\n",
    "\n",
    "åŸç†ï¼šç»´æŠ¤ä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œä»ä¸­éšæœºé€‰æ‹©æ ·æœ¬è¾“å‡º\n",
    "\n",
    "å‚æ•°ï¼šbuffer_sizeæ§åˆ¶shuffleçš„æ•ˆæœï¼ˆè¶Šå¤§è¶Šéšæœºï¼‰\n",
    "\n",
    "ä¿®æ”¹2ï¼šåŒ…è£…åŸå§‹æ•°æ®é›†\n",
    "å°†åŸå§‹çš„ train_data åŒ…è£…åœ¨ ShuffledIterableDataset ä¸­\n",
    "\n",
    "åœ¨ DataLoader ä¸­ä½¿ç”¨ shuffle=Falseï¼ˆå› ä¸ºshuffleå·²ç»åœ¨åŒ…è£…å™¨ä¸­å®ç°ï¼‰\n",
    "\n",
    "ä¿®æ”¹3ï¼šæ·»åŠ äº†å¿…è¦çš„import\n",
    "æ·»åŠ äº† import random\n",
    "\n",
    "æ·»åŠ äº† from torch.utils.data import IterableDataset\n",
    "\n",
    "ğŸ’¡ é€‰æ‹©å»ºè®®\n",
    "å¦‚æœæ•°æ®é›†å¾ˆå¤§ï¼ˆæ— æ³•å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰ï¼šä½¿ç”¨ç¬¬ä¸€ç§æ–¹æ³•ï¼ˆShuffledIterableDatasetï¼‰\n",
    "\n",
    "å¦‚æœæ•°æ®é›†é€‚ä¸­ï¼šä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•ï¼ˆè½¬æ¢ä¸ºlistï¼‰ï¼Œè¿™æ ·æ›´ç®€å•ç›´æ¥\n",
    "\n",
    "å¦‚æœä½¿ç”¨Hugging Face datasetsï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ .to_list() æ–¹æ³•è½¬æ¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
