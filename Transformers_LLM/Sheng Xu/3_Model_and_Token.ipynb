{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a6cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting widgetsnbextension\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Downloading notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter)\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyter) (6.30.1)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Downloading jupyterlab-4.4.7-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from ipykernel->jupyter) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter)\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyterlab->jupyter) (80.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading pywinpty-3.0.0-cp311-cp311-win_amd64.whl.metadata (101 bytes)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading rpds_py-0.27.1-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
      "  Downloading mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter)\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading lark-1.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\zhen-\\anaconda3\\envs\\torch_cuda\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.6 MB/s  0:00:00\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading jupyterlab-4.4.7-py3-none-any.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/12.3 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.8/12.3 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.3 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 13.1 MB/s  0:00:00\n",
      "Downloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Downloading argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 4.5/10.2 MB 20.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 17.7 MB/s  0:00:00\n",
      "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Downloading mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading pywinpty-3.0.0-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 16.3 MB/s  0:00:00\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Downloading lark-1.3.0-py3-none-any.whl (113 kB)\n",
      "Downloading rpds_py-0.27.1-cp311-cp311-win_amd64.whl (228 kB)\n",
      "Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl (17 kB)\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading notebook-7.4.5-py3-none-any.whl (14.3 MB)\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 5.0/14.3 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.9/14.3 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.3 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.3/14.3 MB 23.0 MB/s  0:00:00\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, fastjsonschema, widgetsnbextension, websocket-client, webcolors, uri-template, types-python-dateutil, tinycss2, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, prometheus-client, pandocfilters, overrides, mistune, lark, jupyterlab_widgets, jupyterlab-pygments, jsonpointer, json5, fqdn, defusedxml, bleach, babel, async-lru, terminado, rfc3987-syntax, referencing, beautifulsoup4, arrow, argon2-cffi-bindings, jupyter-server-terminals, jsonschema-specifications, isoduration, ipywidgets, argon2-cffi, jupyter-console, jsonschema, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "\n",
      "   -- -------------------------------------  3/53 [websocket-client]\n",
      "   ----- ----------------------------------  7/53 [tinycss2]\n",
      "   --------- ------------------------------ 13/53 [pywinpty]\n",
      "   ----------- ---------------------------- 15/53 [prometheus-client]\n",
      "   ------------- -------------------------- 18/53 [mistune]\n",
      "   -------------- ------------------------- 19/53 [lark]\n",
      "   ----------------- ---------------------- 23/53 [json5]\n",
      "   ------------------ --------------------- 25/53 [defusedxml]\n",
      "   ------------------- -------------------- 26/53 [bleach]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   -------------------- ------------------- 27/53 [babel]\n",
      "   --------------------- ------------------ 28/53 [async-lru]\n",
      "   ----------------------- ---------------- 31/53 [referencing]\n",
      "   ------------------------ --------------- 33/53 [arrow]\n",
      "   ---------------------------- ----------- 38/53 [ipywidgets]\n",
      "   ---------------------------- ----------- 38/53 [ipywidgets]\n",
      "   ------------------------------ --------- 41/53 [jsonschema]\n",
      "   ------------------------------- -------- 42/53 [nbformat]\n",
      "   -------------------------------- ------- 43/53 [nbclient]\n",
      "   --------------------------------- ------ 45/53 [nbconvert]\n",
      "   --------------------------------- ------ 45/53 [nbconvert]\n",
      "   ---------------------------------- ----- 46/53 [jupyter-server]\n",
      "   ---------------------------------- ----- 46/53 [jupyter-server]\n",
      "   ----------------------------------- ---- 47/53 [notebook-shim]\n",
      "   ------------------------------------ --- 49/53 [jupyter-lsp]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   ------------------------------------- -- 50/53 [jupyterlab]\n",
      "   -------------------------------------- - 51/53 [notebook]\n",
      "   -------------------------------------- - 51/53 [notebook]\n",
      "   -------------------------------------- - 51/53 [notebook]\n",
      "   ---------------------------------------- 53/53 [jupyter]\n",
      "\n",
      "Successfully installed argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.3.0 async-lru-2.0.5 babel-2.17.0 beautifulsoup4-4.13.5 bleach-6.2.0 defusedxml-0.7.1 fastjsonschema-2.21.2 fqdn-1.5.1 ipywidgets-8.1.7 isoduration-20.11.0 json5-0.12.1 jsonpointer-3.0.0 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.7 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab_widgets-3.0.15 lark-1.3.0 mistune-3.1.4 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.4.5 notebook-shim-0.2.4 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.23.1 python-json-logger-3.3.0 pywinpty-3.0.0 referencing-0.36.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 rpds-py-0.27.1 send2trash-1.8.3 soupsieve-2.8 terminado-0.18.1 tinycss2-1.4.0 types-python-dateutil-2.9.0.20250822 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyter ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73921365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了像之前使用 AutoModel 根据 checkpoint 自动加载模型以外，我们也可以直接使用模型对应的 Model 类，例如 BERT 对应的就是 BertModel：\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64f737e",
   "metadata": {},
   "source": [
    "注意，在大部分情况下，我们都应该使用 AutoModel 来加载模型。这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42635d28",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b568b569",
   "metadata": {},
   "source": [
    "所有存储在 HuggingFace Model Hub 上的模型都可以通过 Model.from_pretrained() 来加载权重，参数可以像上面一样是 checkpoint 的名称，也可以是本地路径（预先下载的模型目录），例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3906668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#model = BertModel.from_pretrained(\"./models/bert\") #  not wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac19108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型通过调用 Model.save_pretrained() 函数实现，例如保存加载的 BERT 模型：\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "model.save_pretrained(\"./models/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fede9d9",
   "metadata": {},
   "source": [
    "这会在保存路径下创建两个文件：\n",
    "\n",
    "config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；\n",
    "pytorch_model.bin：又称为 state dictionary，存储模型的权重。\n",
    "简单来说，配置文件记录模型的结构，模型权重记录模型的参数，这两个文件缺一不可。我们自己保存的模型同样通过 Model.from_pretrained() 加载，只需要传递保存目录的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48d793",
   "metadata": {},
   "source": [
    "由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为编码 (Encoding)，其包含两个步骤：\n",
    "\n",
    "使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens；\n",
    "将所有的 token 映射到对应的 token ID。么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5e6698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/bert-base-cased/tokenizer_config.json',\n",
       " './models/bert-base-cased/special_tokens_map.json',\n",
       " './models/bert-base-cased/vocab.txt',\n",
       " './models/bert-base-cased/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分词器的加载与保存与模型相似，使用 Tokenizer.from_pretrained() 和 Tokenizer.save_pretrained() 函数。例如加载并保存 BERT 模型的分词器：\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"./models/bert-base-cased/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdd28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/bert-base-cased/tokenizer_config.json',\n",
       " './models/bert-base-cased/special_tokens_map.json',\n",
       " './models/bert-base-cased/vocab.txt',\n",
       " './models/bert-base-cased/added_tokens.json',\n",
       " './models/bert-base-cased/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样地，我们可以用 AutoModelForSequenceClassification 来加载预训练模型：\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.save_pretrained(\"./models/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1b282",
   "metadata": {},
   "source": [
    "\n",
    "#调用 Tokenizer.save_pretrained() 函数会在保存路径下创建三个文件：\n",
    "\n",
    "special_tokens_map.json：映射文件，里面包含 unknown token 等特殊字符的映射关系；\n",
    "tokenizer_config.json：分词器配置文件，存储构建分词器需要的参数；\n",
    "vocab.txt：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2b3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "# 编码与解码文本\n",
    "# 前面说过，文本编码 (Encoding) 过程包含两个步骤：\n",
    "\n",
    "# 分词：使用分词器按某种策略将文本切分为 tokens；\n",
    "# 映射：将 tokens 转化为对应的 token IDs。\n",
    "# 下面我们首先使用 BERT 分词器来对文本进行分词：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1caafdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "#'using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
    "#可以看到，BERT 分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的 token，例如 “transformer” 会被切分为 “transform” 和 “##er”。\n",
    "\n",
    "#然后，我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26fc8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]\n"
     ]
    }
   ],
   "source": [
    "# 还可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "sequence_ids = tokenizer.encode(sequence)\n",
    "\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00aac8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n",
      "[CLS] Using a Transformer network is simple [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 文本解码 (Decoding) 与编码相反，负责将 token IDs 转换回原来的字符串。注意，解码过程不是简单地将 token IDs 映射回 tokens，还需要合并那些被分为多个 token 的单词。下面我们通过 decode() 函数解码前面生成的 token IDs：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)\n",
    "\n",
    "decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd845f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits:\n",
      " tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 现实场景中，我们往往会同时处理多段文本，而且模型也只接受批 (batch) 数据作为输入，即使只有一段文本，也需要将它组成一个只包含一个样本的 batch，例如：\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\\n\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\\n\", output.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b8b241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Keys:\n",
      " KeysView({'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})\n",
      "\n",
      "Input IDs:\n",
      " tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]])\n",
      "\n",
      "Logits:\n",
      " tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意，上面的代码仅作为演示。实际场景中，我们应该直接使用分词器对文本进行处理，例如对于上面的例子：\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence   = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(\"Inputs Keys:\\n\", tokenized_inputs.keys())\n",
    "print(\"\\nInput IDs:\\n\", tokenized_inputs[\"input_ids\"])\n",
    "\n",
    "output = model(**tokenized_inputs)\n",
    "print(\"\\nLogits:\\n\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc95c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按批输入多段文本产生的一个直接问题就是：batch 中的文本有长有短，而输入张量必须是严格的二维矩形，维度为 \n",
    "# ，即每一段文本编码后的 token IDs 数量必须一样多。例如下面的 ID 列表是无法转换为张量的：\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca07553",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1001db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 因此，在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了，在前面的例子中相信你已经多次见过它了。\n",
    "\n",
    "# Attention Mask\n",
    "# Attention Mask 是一个尺寸与 input IDs 完全相同，且仅由 0 和 1 组成的张量，0 表示对应位置的 token 是填充符，不参与计算。当然，一些特殊的模型结构也会借助 Attention Mask 来遮蔽掉指定的 tokens。\n",
    "\n",
    "# 对于上面的例子，如果我们通过 attention_mask 标出填充的 padding token 的位置，计算结果就不会有问题了：\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "batched_attention_masks = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "outputs = model(\n",
    "    torch.tensor(batched_ids), \n",
    "    attention_mask=torch.tensor(batched_attention_masks))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17da8ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 直接使用分词器\n",
    "# 正如前面所说，在实际使用时，我们应该直接使用分词器来完成包括分词、转换 token IDs、Padding、构建 Attention Mask、截断等操作。例如：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f35cb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-1.5607,  1.6123],\n",
      "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型：\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90f70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# <!-- 编码句子对\n",
    "# 除了对单段文本进行编码以外（batch 只是并行地编码多个单段文本），对于 BERT 等包含“句子对”预训练任务的模型，它们的分词器都支持对“句子对”进行编码，例如： -->\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(inputs)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72789819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,\n",
      "         2200, 2200, 2200, 2146, 1012,  102],\n",
      "        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([3, 18])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence1_list = [\"First sentence.\", \"This is the second sentence.\", \"Third one.\"]\n",
    "sentence2_list = [\"First sentence is short.\", \"The second sentence is very very very long.\", \"ok.\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sentence1_list,\n",
    "    sentence2_list,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokens)\n",
    "print(tokens['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c9a125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhen-\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Prepare input\n",
    "text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Access the underlying DistilBertModel and its embeddings\n",
    "distilbert_model = model.distilbert\n",
    "embeddings_layer = distilbert_model.embeddings\n",
    "\n",
    "# Get the token embeddings\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "token_embeddings = embeddings_layer(input_ids)\n",
    "\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c172f2",
   "metadata": {},
   "source": [
    "分词器还可以通过 return_tensors 参数指定返回的张量格式：\n",
    "* 设为 pt 则返回 PyTorch 张量；\n",
    "* 设为 tf 则返回 TensorFlow 张量，\n",
    "* 设为 np 则返回 NumPy 数组。例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ff3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # PyTorch tensors\n",
    "print(model_inputs)\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\") # NumPy arrays\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89518e5c",
   "metadata": {},
   "source": [
    "编码句子对\n",
    "除了对单段文本进行编码以外（batch 只是并行地编码多个单段文本），对于 BERT 等包含“句子对”预训练任务的模型，它们的分词器都支持对“句子对”进行编码，例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafcee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(inputs)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562e088",
   "metadata": {},
   "source": [
    "+ [CLS] (Classification Token) \n",
    "+ [SEP] (Separator Token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff68e3",
   "metadata": {},
   "source": [
    "5.4 添加 Token\n",
    "实际操作中，我们还经常会遇到输入中需要包含特殊标记符的情况，例如使用 \n",
    " 和 \n",
    " 标记出文本中的实体。由于这些自定义 token 并不在预训练模型原来的词表中，因此直接运用分词器处理就会出现问题。\n",
    "\n",
    "例如直接使用 BERT 分词器处理下面的句子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0443f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', '[', 'en', '##t', '_', 'start', ']', 'cars', '[', 'en', '##t', '_', 'end', ']', 'collided', 'in', 'a', '[', 'en', '##t', '_', 'start', ']', 'tunnel', '[', 'en', '##t', '_', 'end', ']', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence = 'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dabf8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 'end of entity': ['end', 'of', 'entity']\n",
      "Tokenized 'start of entity': ['start', 'of', 'entity']\n",
      "Last two embeddings:\n",
      "tensor([[-0.0490, -0.0195, -0.0482,  ..., -0.0171,  0.0313, -0.0294],\n",
      "        [-0.0146, -0.0251, -0.0270,  ..., -0.0267,  0.0213, -0.0392]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# 假设你已经加载了 tokenizer 和 model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "descriptions = ['start of entity', 'end of entity']\n",
    "\n",
    "# 确保模型在 eval 模式（避免 dropout 等影响）\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, token in enumerate(reversed(descriptions), start=1):\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        print(f\"Tokenized '{token}': {tokenized}\")\n",
    "        \n",
    "        if not tokenized:\n",
    "            print(f\"Warning: '{token}' tokenized to empty list!\")\n",
    "            continue\n",
    "            \n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        # 获取底层 DistilBERT 模型的 embeddings\n",
    "        word_embeddings = model.distilbert.embeddings.word_embeddings.weight\n",
    "        new_embedding = word_embeddings[tokenized_ids].mean(dim=0)\n",
    "        \n",
    "        # 将新嵌入写入特殊 token 位置（如 [unused1], [unused2] 等）\n",
    "        # 注意：DistilBERT 的 vocab 末尾通常有特殊 token（如 [unused999]）\n",
    "        # 你需要确保 -i 位置是可写的（最好使用预留的特殊 token）\n",
    "        model.distilbert.embeddings.word_embeddings.weight[-i, :] = new_embedding.clone().detach()\n",
    "\n",
    "# 查看最后两个 token 的嵌入\n",
    "print(\"Last two embeddings:\")\n",
    "print(model.distilbert.embeddings.word_embeddings.weight[-2:, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929278c",
   "metadata": {},
   "source": [
    "初始化为已有 token 的值\n",
    "更为高级的做法是根据新添加 token 的语义来进行初始化。例如将值初始化为 token 语义描述中所有 token 的平均值，假设新 token \n",
    " 的语义描述为 \n",
    "，那么初始化 \n",
    " 的 embedding 为： \n",
    " \n",
    "\n",
    "这里 \n",
    " 表示预训练模型的 embedding 矩阵。对于上面的例子，我们可以分别为 \n",
    " 和 \n",
    " 编写对应的描述，然后再对它们的值进行初始化：\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
