{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1a2eb7",
   "metadata": {},
   "source": [
    "开箱即用的 pipelines\n",
    "Transformers 库将目前的 NLP 任务归纳为几下几类：\n",
    "\n",
    "文本分类：例如情感分析、句子对关系判断等；\n",
    "对文本中的词语进行分类：例如词性标注 (POS)、命名实体识别 (NER) 等；\n",
    "文本生成：例如填充预设的模板 (prompt)、预测文本中被遮掩掉 (masked) 的词语；\n",
    "从文本中抽取答案：例如根据给定的问题从一段文本中抽取出对应的答案；\n",
    "根据输入文本生成新的句子：例如文本翻译、自动摘要等。\n",
    "Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 pipelines 有：\n",
    "\n",
    "feature-extraction （获得文本的向量化表示）\n",
    "fill-mask （填充被遮盖的词、片段）\n",
    "ner（命名实体识别）\n",
    "question-answering （自动问答）\n",
    "sentiment-analysis （情感分析）\n",
    "summarization （自动摘要）\n",
    "text-generation （文本生成）\n",
    "translation （机器翻译）\n",
    "zero-shot-classification （零训练样本分类）\n",
    "下面我们以常见的几个 NLP 任务为例，展示如何调用这些 pipeline 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e853d1",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/huggingface_hub/v1.0.0.rc2/quick-start#authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a782035",
   "metadata": {},
   "source": [
    "The easiest way to authenticate is to save the token on your machine. You can do that from the terminal using the login() command:\n",
    "\n",
    "hf auth login # in the terminal <<===============================\n",
    "\n",
    "The command will tell you if you are already logged in and prompt you for your token. The token is then validated and saved in your \n",
    "\n",
    "HF_HOME directory (defaults to ~/.cache/huggingface/token). \n",
    "\n",
    "\n",
    "Alternatively, you can programmatically login using login() in a notebook or a script:\n",
    "\n",
    "Copied\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "Any script or library interacting with the Hub will use this token when sending requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ef5ef",
   "metadata": {},
   "source": [
    "Token can be pasted using 'Right-Click'.\n",
    "Enter your token (input will not be visible):\n",
    "\n",
    "Add token as git credential? (Y/n) y\n",
    "Token is valid (permission: read).\n",
    "\n",
    "The token `read` has been saved to C:\\Users\\zhen-\\.cache\\huggingface\\stored_tokens\n",
    "\n",
    "Your token has been saved in your configured git credential helpers (manager).\n",
    "Your token has been saved to C:\\Users\\zhen-\\.cache\\huggingface\\token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869bf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感分析\n",
    "# 借助情感分析 pipeline，我们只需要输入文本，就可以得到其情感标签（积极/消极）以及对应的概率：\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ab0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598046541213989}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9598046541213989}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)\n",
    "results = classifier(\n",
    "  [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8f533",
   "metadata": {},
   "source": [
    "pipeline 模型会自动完成以下三个步骤：\n",
    "\n",
    "将文本预处理为模型可以理解的格式；\n",
    "将预处理好的文本送入模型；\n",
    "对模型的预测值进行后处理，输出人类可以理解的格式。\n",
    "pipeline 会自动选择合适的预训练模型来完成任务。例如对于情感分析，\n",
    "\n",
    "默认就会选择微调好的英文情感模型 \n",
    "distilbert-base-uncased-finetuned-sst-2-english。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c09e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445960879325867, 0.11197640746831894, 0.0434274859726429]}\n"
     ]
    }
   ],
   "source": [
    "# 零训练样本分类\n",
    "# 零训练样本分类 pipeline 允许我们在不提供任何标注数据的情况下自定义分类标签。\n",
    "# 可以看到，pipeline 自动选择了预训练好的 facebook/bart-large-mnli 模型来完成任务。\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\n",
    "\"This is a course about the Transformers library\",\n",
    "candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1258a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to design, test and deploy apps using Android, iOS and Windows. When you are ready to get started, look for our free introductory course covering some of the best Android and Windows apps for building your own apps.'}]\n",
      "[{'generated_text': \"In this course, we will teach you how to create, execute, and use a few of the most common and complex and complex command line utilities. We will also cover building your own utilities such as nginx, apache2, nginx-bootstrap, and more. The course will also cover how to build a custom nginx web server and how to set up a web server in your browser.\\n\\nThe course will cover:\\n\\nSetting up a custom nginx web server and a custom nginx web server\\n\\nHow to set up a custom nginx web server and a custom nginx web server\\n\\nWhat is a custom nginx web server\\n\\nCustom nginx web servers are servers that are used to communicate with other nginx servers. When you use a custom nginx web server, you are authenticating to those nginx servers through an authentication token. A custom nginx web server is not a simple website that just sends messages. For example, if you want to send a message and a message doesn't match a specified URL, you would use a custom nginx web server.\\n\\nFor more information, see Custom Nginx web servers.\\n\\nHow to set up a custom nginx web server\\n\\nTo set up a custom nginx web server, you can use a\"}, {'generated_text': 'In this course, we will teach you how to navigate the web using JavaScript, including how to use Web Components, how to set up Web Components, and how to interact with HTML, JavaScript, CSS, and JavaScript code.\\n\\nYou will be introduced to Web Components, CSS, JavaScript, and JavaScript code. You will learn how to set up your own Web Components, Web Components that are accessible to all web browsers, and Web Components that are distributed in a distributed way. You will learn how to build and use Web Components, Web Components that allow you to create your own Web Components, Web Components that are easier to use, and Web Components that require a bit more code to maintain.\\n\\nYou will learn the basics of using Web Components, Web Components that allow you to provide custom JavaScript to extend your Web Components, Web Components that allow you to extend your Web Components, Web Components that allow you to create and share Web Components, Web Components that allow you to create web components, Web Components that allow you to make your own custom Web Components, Web Components that allow you to create a custom Web Component, and Web Components that allow you to create a custom Web Component.\\n\\nYou will learn how to use Web Components, Web Components that allow you to create or share Web Components, Web Components that allow you to'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "results = generator(\"In this course, we will teach you how to\")\n",
    "print(results)\n",
    "results = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    num_return_sequences=2,\n",
    "    max_length=50\n",
    ") \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a62a4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to integrate and build new applications in the Ruby language. We want you to know that we will be looking for a specific language for you. We will be looking for a specific language for you. The next time you are looking for a language for your work, we are willing to make a commitment to the Ruby language.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}, {'generated_text': 'In this course, we will teach you how to use a computer to perform a task like this.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "results = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a324fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__) # 2.5.1+cu121\n",
    "#2025_10_04_huggingface_transformers_after upgrade to at least 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aff81f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '[CLS] 万 叠 春 山 积 雨 晴 ， 堂 终 夜 深 。 风 撼 林 薄 ， 寒 气 侵 人 衣 。 严 严 大 雅 音 ， 惨 惨 清 商 音 。 灯 花 故 欲 结 ， 酒 尊 良 可 斟 。 幽 人 独 不 眠 ， 起 舞 谁 与 斟 。 此 一 杯 酒 ， 怀 抱 郁 沉 沉 。 古 今 不 同 调 ， 离 别 何 足 斟 。 哉 一 杯 酒 ， 明 日 千 古 心 。 予 亦 何 为 ， 老 矣 难 独 吟 。 此 一 杯 酒 ， 悠 悠 江 上 心 。 怅 岁 云 暮 怀 人 ， 一 曲 ， 朱 颜 绿 ， 江 日 映 青 山 河 月 ， 云 ， 月 青 天 水 ， 金 水 ， 水 。 世 界 球 。 人 事 文 万 世 之 云 之 秤 秤 则 万 象 ， 君 不 二 天 一 寸 ， 月 ， 天 一 卷 ， 一 点 ， 天 一 粒 之 万 。 万 象 ， 一 尘 。 而 不 在 ， 天 之 一 块 ， 天 一 粒 之 一 粒 之 万 古 难 灭 。 君 万 物 之 所 一 尘 。 物 之 所 不 可 该 。 浩 然 时 之 一 尘 。 无 泯 然 ， 不 动 而 无 物 ， ， ， 天 不 该 。 不 动 而'}, {'generated_text': '[CLS] 万 叠 春 山 积 雨 晴 ， 友 构 幽 宇 。 长 啸 天 风 来 回 首 ， 群 峰 拱 揖 如 宾 主 。 高 标 可 望 不 可 攀 ， 流 云 荡 漾 青 烟 鬟 。 山 中 白 云 长 媚 妩 ， 何 事 高 飞 不 能 雨 。 馀 坐 对 两 苍 松 ， 流 水 孤 村 劳 逸 踪 。 我 欲 从 之 不 可 得 ， 云 中 之 人 在 何 许 。 哉 此 意 可 深 思 ， 归 耕 未 得 南 山 芝 。 哉 青 山 如 白 云 ， 白 云 何 日 复 归 来 。 山 云 ， 云 似 水 ， 水 之 之 。 云 云 ， 云 之 云 云 之 云 兮 ， 云 云 之 云 可 ， 吾 之 云 ， 空 。 云 云 云 非 ， 云 云 云 云 如 之 云 云 云 不 可 萦 。 我 之 。 可 可 ， 溪 ， 万 山 之 云 之 。 云 ， 高 ， 我 之 云 似 ， 云 ， 云 兮 不 可 。 云 。 我 兮 之 如 之 不 可 。 云 之 云 不 可 无 边 兮 在 ， 云 如 之 尘 何 如 之 之 自 兮 云 不 可 见 。 一 之 不 可 可 寄 尘 。 云 ， 何 如 之 。 云 。 云 兮 何 如'}]\n"
     ]
    }
   ],
   "source": [
    "# 还可以通过左边的语言 tag 选择其他语言的模型。例如加载专门用于生成中文古诗的 gpt2-chinese-poem 模型：\n",
    "#pip install --upgrade torch torchvision torchaudio\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"uer/gpt2-chinese-poem\")\n",
    "results = generator(\n",
    "    \"[CLS] 万 叠 春 山 积 雨 晴 ，\",\n",
    "    max_length=40,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1152e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 PyTorch 版本: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, \n",
    "# we now require users to upgrade torch to at least v2.6 in order to use the function. \n",
    "# This version restriction does not apply when loading files with safetensors.\n",
    "import torch\n",
    "print(f\"当前 PyTorch 版本: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf4dc8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# nvcc --version  #  in terminal to check the version of CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee100c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.19619786739349365, 'token': 30412, 'token_str': ' mathematical', 'sequence': 'This course will teach you all about mathematical models.'}, {'score': 0.040527161210775375, 'token': 38163, 'token_str': ' computational', 'sequence': 'This course will teach you all about computational models.'}]\n"
     ]
    }
   ],
   "source": [
    "# 给定一段部分词语被遮盖掉 (masked) 的文本，使用预训练模型来预测能够填充这些位置的词语。\n",
    "# 与前面介绍的文本生成类似，这个任务其实也是先构建模板然后运用模型来完善模板，称为填充模板 (Cloze Prompt)。了解更多详细信息可以查看《Prompt 方法简介》。\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "results = unmasker(\"This course will teach you all about <mask> models.\", top_k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11272071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': np.float32(0.9981694), 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': np.float32(0.9796019), 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': np.float32(0.9932105), 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\torch_cuda\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 命名实体识别 (NER<=====) pipeline 负责从文本中抽取出指定类型的实体，例如人物、地点、组织等等。\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "results = ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e3b67b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6949766278266907, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}\n"
     ]
    }
   ],
   "source": [
    "# 自动问答 pipeline 可以根据给定的上下文回答问题，例如：\n",
    "# 这里的自动问答 pipeline 实际上是一个抽取式问答模型，即从给定的上下文中抽取答案，而不是生成答案。\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "answer = question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa938b",
   "metadata": {},
   "source": [
    "根据形式的不同，自动问答 (QA) 系统可以分为三种：\n",
    "\n",
    "抽取式 QA (extractive QA)：假设答案就包含在文档中，因此直接从文档中抽取答案；\n",
    "多选 QA (multiple-choice QA)：从多个给定的选项中选择答案，相当于做阅读理解题；\n",
    "无约束 QA (free-form QA)：直接生成答案文本，并且对答案文本格式没有任何限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c77ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}]\n"
     ]
    }
   ],
   "source": [
    "#自动摘要 pipeline 旨在将长文本压缩成短文本，并且还要尽可能保留原文的主要信息，例如：\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "results = summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "    \"\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1762b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598045349121094}]\n"
     ]
    }
   ],
   "source": [
    "#这些 pipeline 背后做了什么？\n",
    "#这些简单易用的 pipeline 模型实际上封装了许多操作，下面我们就来了解一下它们背后究竟做了啥。\n",
    "# 以第一个情感分析 pipeline 为例，我们运行下面的代码\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7be338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 就会得到结果：\n",
    "\n",
    "# [{'label': 'POSITIVE', 'score': 0.9598048329353333}]\n",
    "\n",
    "# 实际上它的背后经过了三个步骤：\n",
    "\n",
    "# 预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式；\n",
    "# 将处理好的输入送入模型；\n",
    "# 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61304",
   "metadata": {},
   "source": [
    "因为神经网络模型无法直接处理文本，因此首先需要通过预处理环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：\n",
    "\n",
    "将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\n",
    "根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）；\n",
    "根据模型的需要，添加一些额外的输入。\n",
    "我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作，如果对要使用的模型不熟悉，可以通过 Model Hub 查询。这里我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n",
    "\n",
    "情感分析 pipeline 的默认 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english，下面我们手工下载并调用其分词器：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d4f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "422459ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将预处理好的输入送入模型\n",
    "# 预训练模型的下载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。下面我们手工下载这个 distilbert-base 模型：\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f0bf1",
   "metadata": {},
   "source": [
    "预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务，例如送入到分类头中完成文本分类任务。\n",
    "\n",
    "其实前面我们举例的所有 pipelines 都具有类似的模型结构，只是模型的最后一部分会使用不同的 head 以完成对应的任务。\n",
    "\n",
    "transformer_and_head\n",
    "\n",
    "Transformers 库封装了很多不同的结构，常见的有：\n",
    "\n",
    "- Model （返回 hidden states）\n",
    "- ForCausalLM （用于条件语言模型）\n",
    "- ForMaskedLM （用于遮盖语言模型）\n",
    "- ForMultipleChoice （用于多选任务）\n",
    "- ForQuestionAnswering （用于自动问答任务）\n",
    "- ForSequenceClassification （用于文本分类任务）\n",
    "- ForTokenClassification （用于 token 分类任务，例如 NER）\n",
    "Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c66ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# 我们可以打印出这里使用的 distilbert-base 模型的输出维度：\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb0046",
   "metadata": {},
   "source": [
    "- Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，\n",
    "- 其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；\n",
    "- Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；\n",
    "- Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a522b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "无序列表：使用 -、+ 或 * 创建项目符号。\n",
    "有序列表：使用数字加点（如 1.）创建编号列表。\n",
    "嵌套列表：通过缩进实现子项目。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b724059",
   "metadata": {},
   "source": [
    "- 主项目 1\n",
    "  - 子项目 1.1\n",
    "  - 子项目 1.2\n",
    "- 主项目 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transformers 模型的输出格式类似 namedtuple 或字典，可以像上面那样通过属性访问，也可以通过键（outputs[\"last_hidden_state\"]），甚至索引访问（outputs[0]）。\n",
    "\n",
    "# 对于情感分析任务，很明显我们最后需要使用的是一个文本分类 head。因此，实际上我们不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification # <<================\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint) # <<================\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b3c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 由于模型的输出只是一些数值，因此并不适合人类阅读。例如我们打印出上面例子的输出：\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c85bf1",
   "metadata": {},
   "source": [
    "模型对第一个句子输出 \n",
    "，对第二个句子输出 \n",
    "，它们并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) # 计算概率 softmax \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6626e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02201370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
