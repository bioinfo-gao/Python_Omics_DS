{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64996767",
   "metadata": {},
   "source": [
    "Transformers 库建立在 Pytorch 框架之上（Tensorflow 的版本功能并不完善），\n",
    "虽然官方宣称使用 Transformers 库并不需要掌握 Pytorch 知识，但是实际上我们还是需要通过 Pytorch 的 \n",
    "DataLoader 类来加载数据、使用 Pytorch 的优化器对模型参数进行调整等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量 (Tensor) 是深度学习的基础，例如常见的 0 维张量称为标量 (scalar)、1 维张量称为向量 (vector)、2 维张量称为矩阵 (matrix)。Pytorch 本质上就是一个基于张量的数学计算工具包，它提供了多种方式来创建张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec0817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.9185e-31,  1.3116e-42,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.empty(2, 3) # empty tensor (uninitialized), shape (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7d4f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0597, 0.7948, 0.3898],\n",
       "        [0.4318, 0.8445, 0.8419]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3) # random tensor, each value taken from [0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6c73a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2076,  0.9583, -0.4509],\n",
       "        [-0.6981, -0.3623, -0.6361]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3) # random tensor, each value taken from standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43d9386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, dtype=torch.long) # long integer zero tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501271d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, dtype=torch.double) # double float zero tensor\n",
    "# tensor([[0., 0., 0.],\n",
    "#         [0., 0., 0.]], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39667b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)\n",
    "torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f67ad13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.8000, 2.1000],\n",
       "        [8.6000, 4.0000, 2.4000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = [[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]]\n",
    "torch.tensor(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b9fc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 3.8000, 2.1000],\n",
       "        [8.6000, 4.0000, 2.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.array([[1.0, 3.8, 2.1], [8.6, 4.0, 2.4]])\n",
    "torch.from_numpy(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856479ec",
   "metadata": {},
   "source": [
    "     PyTorch with CUDA Support \n",
    "     conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6540a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "# this code asures that pytotch can access the GPU\n",
    "# shift the conda env pytorh_GPU_cuda to the front\n",
    "import torch\n",
    "#print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e7230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.6.0+cu124\n",
      "CUDA available True\n",
      "CUDA version 12.4\n",
      "GPU NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"PyTorch\", torch.__version__) # 2.80+CPU is CPU , 2.6.0cu is CUDA\n",
    "print(\"CUDA available\", torch.cuda.is_available())\n",
    "print(\"CUDA version\", torch.version.cuda)\n",
    "print(\"GPU\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7838339",
   "metadata": {},
   "source": [
    "PyTorch build: 2.8.0+cu126\n",
    "Python: 3.11.13 64bit\n",
    "CUDA runtime (nvcc): Cuda compilation tools, release 12.6, V12.6.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e881d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch build: 2.6.0+cu124\n",
      "Python: 3.11.13 64bit\n",
      "CUDA runtime (nvcc): Cuda compilation tools, release 12.6, V12.6.85\n"
     ]
    }
   ],
   "source": [
    "import torch, platform, subprocess, sys\n",
    "print(\"PyTorch build:\", torch.__version__)          #PyTorch now sees the CUDA 12.1 wheel (+cu121)\n",
    "print(\"Python:\", sys.version.split()[0], platform.architecture()[0])\n",
    "try:\n",
    "    print(\"CUDA runtime (nvcc):\", subprocess.check_output([\"nvcc\",\"--version\"], text=True).split(\"\\n\")[3])\n",
    "except FileNotFoundError:\n",
    "    print(\"CUDA runtime (nvcc): not found  ← this is OK, wheels bundle their own\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225bf99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7627, 0.1800, 0.5982],\n",
       "        [0.5238, 0.3279, 0.6467]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上面这些方式创建的张量会存储在内存中并使用 CPU 进行计算，如果想要调用 GPU 计算，需要直接在 GPU 中创建张量或者将张量送入到 GPU 中：\n",
    "\n",
    "torch.rand(2, 3).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Random tensor on GPU: tensor([0.1086], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(  \"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(   \"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "x = torch.rand(1).cuda()\n",
    "print(\"Random tensor on GPU:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392c114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4040, 0.3405, 0.9690],\n",
       "        [0.2856, 0.7073, 0.5478]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2300, 0.5589, 0.6929],\n",
       "        [0.0508, 0.3757, 0.6551]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 3).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249baecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行 view 操作的张量必须是连续的 (contiguous)，可以调用 is_conuous 来判断张量是否连续；\n",
    "# 如果非连续，需要先通过 contiguous 函数将其变为连续的。\n",
    "# 也可以直接调用 Pytorch 新提供的 reshape 函数，\n",
    "# 它与 view 功能几乎一致，并且能够自动处理非连续张量。\n",
    "\n",
    "# 转置 transpose 交换张量中的两个维度，参数为相应的维度：\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3c1970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c21fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]]) torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 交换维度 permute 与 transpose 函数每次只能交换两个维度不同，permute 可以直接设置新的维度排列方式：\n",
    "\n",
    "x = torch.tensor([[[1, 2, 3], [4, 5, 6]]])\n",
    "print(x, x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d83249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 4]],\n",
      "\n",
      "        [[2, 5]],\n",
      "\n",
      "        [[3, 6]]]) torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = x.permute(2, 0, 1)\n",
    "print(x, x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b3a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([4, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 4) # shape (1,3) \n",
    "y = torch.arange(4, 6) # shape (1,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ac4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 4).view(3, 1) # shape (3,1) \n",
    "y = torch.arange(4, 6).view(1, 2) # shape (1,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制\n",
    "# 前面我们都是假设参与运算的两个张量形状相同。在有些情况下，即使两个张量形状不同，\n",
    "# 也可以通过广播机制 (broadcasting mechanism) \n",
    "# 对其中一个或者同时对两个张量的元素进行复制，使得它们形状相同，然后再执行按元素计算。\n",
    "\n",
    "# 例如，我们生成两个形状不同的张量：\n",
    "\n",
    "x = torch.arange(1, 4).view(3, 1) # shape (3,1) \n",
    "y = torch.arange(4, 6).view(1, 2) # shape (1,2)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [6, 7],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    " print(x + y) \n",
    "# shape (3,2), \n",
    "# x along dim 1 is copied twice, \n",
    "# y along dim 0 is copied three times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c061502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).view(2,3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4afa2eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).view(3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8aced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x[1, 3] # element at row index 1 (second row), column 3 (4th column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d4cafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " x[1:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9db6690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  6, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cf555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3],\n",
       "        [ 6,  7],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 卸载现有 Pillow（无论用 pip 还是 conda 装的）\n",
    "# # run in in terminal is better by ZHEN . because jupyter sometimes has issue with conda yes not show in cell \n",
    "# conda uninstall pillow -y \n",
    "# pip uninstall pillow  # 确保彻底清除!\n",
    "\n",
    "# # 2. 用 conda 重新安装（自动解决 DLL 依赖）\n",
    "# conda install pillow -c conda-forge -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "176ada6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())  # 应输出 ('64bit', 'WindowsPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abf54ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(Image.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829b053",
   "metadata": {},
   "source": [
    "cannot import name 'datasets' from 'torchvision' (unknown location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90255efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhen-\\AppData\\Local\\Temp\\ipykernel_25840\\1164771907.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "print(pkg_resources.get_distribution(\"torchvision\").version)\n",
    "# # 从 pytorch 官方频道安装（自动匹配 CUDA 版本）\n",
    "#conda install torchvision -c pytorch -c nvidia\n",
    "# conda update torchvision -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b5bdf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e187b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:07<00:00, 3.61MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 235kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:04<00:00, 1.00MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n",
      "torch.Size([28, 28])\n",
      "Label: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data       import DataLoader\n",
    "from torchvision            import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "print(img.shape)\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a5f3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "#conda activate torch_cuda\n",
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f69e9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        assert end > start\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.end))\n",
    "\n",
    "ds = MyIterableDataset(start=3, end=7) # [3, 4, 5, 6]\n",
    "# Single-process loading\n",
    "print(list(DataLoader(ds, num_workers=0)))\n",
    "# # Directly doing multi-process loading\n",
    "# print(list(DataLoader(ds, num_workers=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffb892",
   "metadata": {},
   "source": [
    "构建模型\n",
    "我们还是以前面加载的 FashionMNIST 数据库为例，构建一个神经网络模型来完成图像分类。模型同样继承自 nn.Module 类，通过 __init__() 初始化模型中的层和参数，在 forward() 中定义模型的操作，例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4b9962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ce654",
   "metadata": {},
   "outputs": [],
   "source": [
    "可以看到，我们构建的模型首先将二维图像通过 Flatten 层压成一维向量，然后经过两个带有 ReLU 激活函数的全连接隐藏层，最后送入到一个包含 10 个神经元的分类器以完成 10 分类任务。我们还通过在最终输出前添加 Dropout 层来缓解过拟合。\n",
    "\n",
    "最终我们构建的模型会输出一个 10 维向量（每一维对应一个类别的预测值），与先前介绍过的 pipeline 模型一样，这里输出的是 logits 值，我们需要再接一个 Softmax 层来计算最终的概率值。下面我们构建一个包含四个伪二维图像的 mini-batch 来进行预测：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "torch.Size([4, 10])\n",
      "Predicted class: tensor([1, 1, 4, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # 28*28=784 input layer size to 512 hidden layer size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),   # 512 hidden layer size to 256 hidden layer size \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),    # 256 hidden layer size to 10 output layer size\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)       # flatten input tensor from 28x28 to 784\n",
    "        logits = self.linear_relu_stack(x)  # pass through the layers\n",
    "        return logits \n",
    "\n",
    "model = NeuralNetwork().to(device) # send the model to GPU \n",
    "\n",
    "# create a mini-batch of 4 random  input images \n",
    "X = torch.rand(4, 28, 28, device=device) # create a mini-batch of 4 random 28x28 images on GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logits = model(X)\n",
    "\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab.size())\n",
    "y_pred = pred_probab.argmax(-1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f97b3",
   "metadata": {},
   "source": [
    "优化模型参数\n",
    "在准备好数据、搭建好模型之后，我们就可以开始训练和测试（验证）模型了。正如前面所说，模型训练是一个迭代的过程，每一轮 epoch 迭代中模型都会对输入样本进行预测，然后对预测结果计算损失 (loss)，并求 loss 对每一个模型参数的偏导，最后使用优化器更新所有的模型参数。\n",
    "\n",
    "损失函数 (Loss function) 用于度量预测值与答案之间的差异，模型的训练过程就是最小化损失函数。Pytorch 实现了很多常见的损失函数，例如用于回归任务的均方误差 (Mean Square Error) nn.MSELoss、用于分类任务的负对数似然 (Negative Log Likelihood) nn.NLLLoss、同时结合了 nn.LogSoftmax 和 nn.NLLLoss 的交叉熵损失 (Cross Entropy) nn.CrossEntropyLoss 等。\n",
    "\n",
    "优化器 (Optimization) 使用特定的优化算法（例如随机梯度下降），通过在每一个训练阶段 (step) 减少（基于一个 batch 样本计算的）模型损失来调整模型参数。Pytorch 实现了很多优化器，例如 SGD、ADAM、RMSProp 等。\n",
    "\n",
    "每一轮迭代 (Epoch) 实际上包含了两个步骤：\n",
    "\n",
    "训练循环 (The Train Loop) 在训练集上进行迭代，尝试收敛到最佳的参数；\n",
    "验证/测试循环 (The Validation/Test Loop) 在测试/验证集上进行迭代以检查模型性能有没有提升。\n",
    "具体地，在训练循环中，优化器通过以下三个步骤进行优化：\n",
    "\n",
    "调用 optimizer.zero_grad() 重设模型参数的梯度。默认情况下梯度会进行累加，为了防止重复计算，在每个训练阶段开始前都需要清零梯度；\n",
    "通过 loss.backwards() 反向传播预测结果的损失，即计算损失对每一个参数的偏导；\n",
    "调用 optimizer.step() 根据梯度调整模型的参数。\n",
    "下面我们选择交叉熵作为损失函数、选择 AdamW 作为优化器，完整的训练循环和测试循环实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8f7ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.737276  [ 6400/60000]\n",
      "loss: 0.988355  [12800/60000]\n",
      "loss: 0.515212  [19200/60000]\n",
      "loss: 0.957792  [25600/60000]\n",
      "loss: 0.488365  [32000/60000]\n",
      "loss: 0.663733  [38400/60000]\n",
      "loss: 0.874533  [44800/60000]\n",
      "loss: 0.499112  [51200/60000]\n",
      "loss: 0.520403  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.449959 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.556642  [ 6400/60000]\n",
      "loss: 0.709385  [12800/60000]\n",
      "loss: 0.537429  [19200/60000]\n",
      "loss: 0.929084  [25600/60000]\n",
      "loss: 0.405274  [32000/60000]\n",
      "loss: 0.591053  [38400/60000]\n",
      "loss: 1.109954  [44800/60000]\n",
      "loss: 0.535239  [51200/60000]\n",
      "loss: 0.351561  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.429587 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.620890  [ 6400/60000]\n",
      "loss: 0.447753  [12800/60000]\n",
      "loss: 0.644886  [19200/60000]\n",
      "loss: 0.663515  [25600/60000]\n",
      "loss: 0.502192  [32000/60000]\n",
      "loss: 0.432372  [38400/60000]\n",
      "loss: 0.826991  [44800/60000]\n",
      "loss: 0.542793  [51200/60000]\n",
      "loss: 0.328057  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392526 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.458885  [ 6400/60000]\n",
      "loss: 0.534319  [12800/60000]\n",
      "loss: 0.416348  [19200/60000]\n",
      "loss: 0.698710  [25600/60000]\n",
      "loss: 0.447988  [32000/60000]\n",
      "loss: 0.483686  [38400/60000]\n",
      "loss: 0.790767  [44800/60000]\n",
      "loss: 0.577412  [51200/60000]\n",
      "loss: 0.534908  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.373972 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.468314  [ 6400/60000]\n",
      "loss: 0.548004  [12800/60000]\n",
      "loss: 0.532581  [19200/60000]\n",
      "loss: 0.751078  [25600/60000]\n",
      "loss: 0.425540  [32000/60000]\n",
      "loss: 0.516235  [38400/60000]\n",
      "loss: 0.802763  [44800/60000]\n",
      "loss: 0.501112  [51200/60000]\n",
      "loss: 0.459891  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.355732 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.449234  [ 6400/60000]\n",
      "loss: 0.640149  [12800/60000]\n",
      "loss: 0.539236  [19200/60000]\n",
      "loss: 0.577948  [25600/60000]\n",
      "loss: 0.404775  [32000/60000]\n",
      "loss: 0.342353  [38400/60000]\n",
      "loss: 0.720294  [44800/60000]\n",
      "loss: 0.401383  [51200/60000]\n",
      "loss: 0.455134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.355246 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.537414  [ 6400/60000]\n",
      "loss: 0.491099  [12800/60000]\n",
      "loss: 0.413754  [19200/60000]\n",
      "loss: 0.669877  [25600/60000]\n",
      "loss: 0.329018  [32000/60000]\n",
      "loss: 0.344221  [38400/60000]\n",
      "loss: 0.694497  [44800/60000]\n",
      "loss: 0.303815  [51200/60000]\n",
      "loss: 0.271817  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.358785 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.533517  [ 6400/60000]\n",
      "loss: 0.480951  [12800/60000]\n",
      "loss: 0.354994  [19200/60000]\n",
      "loss: 0.533918  [25600/60000]\n",
      "loss: 0.262957  [32000/60000]\n",
      "loss: 0.313119  [38400/60000]\n",
      "loss: 0.563771  [44800/60000]\n",
      "loss: 0.329874  [51200/60000]\n",
      "loss: 0.462439  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.355463 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.382754  [ 6400/60000]\n",
      "loss: 0.477310  [12800/60000]\n",
      "loss: 0.402336  [19200/60000]\n",
      "loss: 0.490591  [25600/60000]\n",
      "loss: 0.413669  [32000/60000]\n",
      "loss: 0.469713  [38400/60000]\n",
      "loss: 0.832029  [44800/60000]\n",
      "loss: 0.487817  [51200/60000]\n",
      "loss: 0.304005  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.358337 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.362968  [ 6400/60000]\n",
      "loss: 0.467170  [12800/60000]\n",
      "loss: 0.397390  [19200/60000]\n",
      "loss: 0.491022  [25600/60000]\n",
      "loss: 0.321436  [32000/60000]\n",
      "loss: 0.361106  [38400/60000]\n",
      "loss: 0.703770  [44800/60000]\n",
      "loss: 0.422955  [51200/60000]\n",
      "loss: 0.363082  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.358670 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU <<====\n",
    "\n",
    "print(f'Using {device} device')\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10 #3\n",
    "\n",
    "# datasets -> dataloaders \n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device) # send the model to GPU \n",
    "\n",
    "# Training loop \n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Test loop\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim=-1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# span style=\"color: red;\">This text is red.</span> set color red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c8426",
   "metadata": {},
   "source": [
    "可以看到，通过 3 轮迭代 (Epoch)，模型在训练集上的损失逐步下降、在测试集上的准确率逐步上升，证明优化器成功地对模型参数进行了调整，而且没有出现过拟合。\n",
    "\n",
    "<span style=\"color: red;\">注意：一定要在预测之前调用 model.eval() .</span>\n",
    "\n",
    "<span style=\"color: orange;\">方法将 dropout 层和 batch normalization 层设置为评估模式， </span>\n",
    "\n",
    "<span style=\"color: cyan;\"> 否则会产生不一致的预测结果。</span>\n",
    "\n",
    "\n",
    "4. 保存及加载模型\n",
    "在之前1的文章中，我们介绍过模型类 Model 的保存以及加载方法，但如果我们只是将预训练模型作为一个模块（例如作为编码器），那么最终的完整模型就是一个自定义 Pytorch 模型，它的保存和加载就必须使用 Pytorch 预设的接口。\n",
    "\n",
    "保存和加载模型权重\n",
    "Pytorch 模型会将所有参数存储在一个状态字典 (state dictionary) 中，可以通过 Model.state_dict() 加载。Pytorch 通过 torch.save() 保存模型权重：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "为了加载保存的权重，我们首先需要创建一个结构完全相同的模型实例，然后通过 Model.load_state_dict() 函数进行加载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1eb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "保存和加载完整模型\n",
    "上面存储模型权重的方式虽然可以节省空间，但是加载前需要构建一个结构完全相同的模型实例来承接权重。\n",
    "如果我们希望在存储权重的同时，也一起保存模型结构，就需要将整个模型传给 torch.save() ：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model, 'model.pth')\n",
    "这样就可以直接从保存的文件中加载整个模型（包括权重和结构）：\n",
    "\n",
    "model = torch.load('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
