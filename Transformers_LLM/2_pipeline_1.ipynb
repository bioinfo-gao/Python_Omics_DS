{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1a2eb7",
   "metadata": {},
   "source": [
    "开箱即用的 pipelines\n",
    "Transformers 库将目前的 NLP 任务归纳为几下几类：\n",
    "\n",
    "文本分类：例如情感分析、句子对关系判断等；\n",
    "对文本中的词语进行分类：例如词性标注 (POS)、命名实体识别 (NER) 等；\n",
    "文本生成：例如填充预设的模板 (prompt)、预测文本中被遮掩掉 (masked) 的词语；\n",
    "从文本中抽取答案：例如根据给定的问题从一段文本中抽取出对应的答案；\n",
    "根据输入文本生成新的句子：例如文本翻译、自动摘要等。\n",
    "Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 pipelines 有：\n",
    "\n",
    "feature-extraction （获得文本的向量化表示）\n",
    "fill-mask （填充被遮盖的词、片段）\n",
    "ner（命名实体识别）\n",
    "question-answering （自动问答）\n",
    "sentiment-analysis （情感分析）\n",
    "summarization （自动摘要）\n",
    "text-generation （文本生成）\n",
    "translation （机器翻译）\n",
    "zero-shot-classification （零训练样本分类）\n",
    "下面我们以常见的几个 NLP 任务为例，展示如何调用这些 pipeline 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感分析\n",
    "# 借助情感分析 pipeline，我们只需要输入文本，就可以得到其情感标签（积极/消极）以及对应的概率：\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)\n",
    "results = classifier(\n",
    "  [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8f533",
   "metadata": {},
   "source": [
    "pipeline 模型会自动完成以下三个步骤：\n",
    "\n",
    "将文本预处理为模型可以理解的格式；\n",
    "将预处理好的文本送入模型；\n",
    "对模型的预测值进行后处理，输出人类可以理解的格式。\n",
    "pipeline 会自动选择合适的预训练模型来完成任务。例如对于情感分析，默认就会选择微调好的英文情感模型 distilbert-base-uncased-finetuned-sst-2-english。m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c09e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9fcc72c76d4b72adf2d6fd373792a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\pytorch_GPU_cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhen-\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9f1b4f80b2406893301a6ba3c31602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29a368c07734475a97b92cd3f08b834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5222cc9269c4e44a162980ccfa66685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0447404184240138604c651101e3db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcab67fb5124c418eff9857f994e851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445960879325867, 0.11197640746831894, 0.0434274859726429]}\n"
     ]
    }
   ],
   "source": [
    "# 零训练样本分类\n",
    "# 零训练样本分类 pipeline 允许我们在不提供任何标注数据的情况下自定义分类标签。\n",
    "# 可以看到，pipeline 自动选择了预训练好的 facebook/bart-large-mnli 模型来完成任务。\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\n",
    "\"This is a course about the Transformers library\",\n",
    "candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1258a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e9a269b5c54816a11c9f27fe59fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\pytorch_GPU_cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhen-\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b190c257bf44338159fb942b6ddacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4152689b1c04738b01c752158defe9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf4c79ac4844362894c9688c319b7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde4c917749745fdbf3c6fff882ec0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c7854640be46ee95439a3dfe1fcd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6b3bc1ba01450e83dcd6919d3925bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will find the tools to create powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will discover the tools to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will discover the tools to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will learn how to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will learn how to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.\\n\\nIn this course, you will learn how to create powerful and powerful virtual machines using the same software and hardware. You will work on multiple scenarios to create powerful and powerful virtual machines.'}]\n",
      "[{'generated_text': \"In this course, we will teach you how to create a simple and efficient user interface that allows you to create a new user experience with virtually no effort. You will learn how to create a user profile on any website that you want to use, and how to create an account with a mobile app. Once you have successfully created an account, you'll be able to add an account to your existing account.\\n\\nYou can join this class now, or simply enroll to our free trial (we're currently only accepting users of the free trial).\\n\\nIf you're interested in joining the class and want the full course, take a look at the full course description.\\n\\nWhat's New\\n\\nThe class takes place at the University of Alberta.\\n\\nThe class includes a free downloadable version of the course, and a downloadable version of the course manual.\\n\\nLearn more about the class here.\"}, {'generated_text': 'In this course, we will teach you how to use a wide variety of technologies to create custom, custom applications.\\n\\nWe will show you how to create a web application using MongoDB, Apache, and MySQL. We will use the same principles in our tutorial with PHP.\\n\\nWe will show you how to use the MongoDB and Apache libraries to build your own applications using MongoDB.\\n\\nWe will show you how to load images from your favorite web server using the MySQL and MongoDB libraries.\\n\\nWe will show you how to use the SQLite engine to create SQLite-related applications using the MongoDB and Apache libraries.\\n\\nWe will show you how to use the Apache web server library to create RESTful applications using MongoDB and Apache.\\n\\nWe will show you how to use MongoDB to create RESTful applications using MongoDB and Apache.\\n\\nWe will show you how to use MongoDB to create RESTful applications using MongoDB and Apache.\\n\\nWe will show you how to use MongoDB to create RESTful applications using MongoDB and Apache.\\n\\nWe will show you how to use MongoDB and Apache to create RESTful applications using MongoDB and Apache.\\n\\nWe will show you how to use MongoDB and Apache to create RESTful applications using Mongo'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "results = generator(\"In this course, we will teach you how to\")\n",
    "print(results)\n",
    "results = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    num_return_sequences=2,\n",
    "    max_length=50\n",
    ") \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a62a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95706572c7594f6989068652d10df53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\pytorch_GPU_cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhen-\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb659348c3c424eb7f7e5c99ed6d3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb9aada29f94a4e83d8c05fa689d419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6040b4c340474d92226edcf44d4b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb60766833a47cab03d6eaee3044c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df66f4a3fc7a44cb8c1e5a7bf4195044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c7547ec41a4bdeb8ea89be3065a468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"In this course, we will teach you how to make simple and effective, and not make a bunch of fancy fancy things that you can't do. The only way to do it is to think about ways to make something very simple.\\n\\n\\nThis course will give you a basic idea of how to make everything simple.\\nAfter you've completed this course, you'll need to make your own.\\nBefore you start making, you'll need to know how to make it easy.\\nFirst, you will need to find a few examples.\\nExample 1:\\nImagine a simple video tutorial.\\nFirst, imagine how to make a simple video tutorial.\\nOnce you've finished it, you can use a few examples.\\nExample 2:\\nImagine another simple video tutorial.\\nNow, you might want to use a few examples.\\nSo you can make this app simple, but it's not easy.\\nYou could start by using a few examples.\\nExample 3:\\nImagine a simple video tutorial.\\nNow, you might need to use a few examples.\\nNow, you might want to use a few examples.\\nA quick example.\\nNow, you might want to use a few examples.\\nNow, you might want to use a few examples.\\nNow, you might want to\"}, {'generated_text': 'In this course, we will teach you how to use and use the power of a hand.\\n\\n\\n\\n\\nThe hand is a very useful tool to build a hand.\\n\\nAs a free hand, you can use it to build a machine to create your own hands. This is what this course will teach you.\\nThis course will teach you how to use and use the power of a hand.\\nThis course will teach you how to use and use the power of a hand.\\nIn this course, you will learn a very useful tool to build a hand.\\nIn this course, you will learn a very useful tool to build a hand.\\nThis course will teach you how to use and use the power of a hand.\\nIn this course, you will learn a very useful tool to build a hand. In this course, you will learn a very useful tool to build a hand.\\nIn this course, you will learn a very useful tool to build a hand. In this course, you will learn a very useful tool to build a hand.\\nIn this course, you will learn a very useful tool to build a hand.\\nIn this course, you will learn a very useful tool to build a hand.\\nIn this course, you will learn a very useful tool to build a'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "results = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aff81f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf503b3d49245038bd5ba9c15b9284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhen-\\anaconda3\\envs\\pytorch_GPU_cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zhen-\\.cache\\huggingface\\hub\\models--uer--gpt2-chinese-poem. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f465bc58ad2349bba55c23855a0a2ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7296eb71e6425a9984f32c7e73a10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '[CLS] 万 叠 春 山 积 雨 晴 ， 望 山 光 紫 。 山 花 发 杜 鹃 啼 血 ， 山 头 日 日 悲 风 起 。 萦 翠 带 花 绸 缪 。 彼 远 游 人 ， 一 别 三 千 秋 。 何 年 此 结 彼 ， 一 旦 埋 荒 洲 。 我 魂 亦 归 ， 当 与 尔 同 游 。 相 思 不 相 见 ， 独 上 古 城 头 。 人 不 可 见 ， 行 云 去 悠 悠 。 哉 彼 流 水 ， 东 去 何 时 休 。 兹 老 兄 弟 ， 生 死 同 衾 裯 。 骨 肉 为 兄 弟 土 壤 壤 壤 壤 壤 壤 ， 魂 气 化 作 灰 槁 ， 魂 气 结 散 作 邻 。 魂 散 作 室 中 夜 夜 台 ， 寒 风 吹 作 曙 ， 月 。 我 屋 角 ， 日 月 轮 转 月 月 兮 星 河 流 光 。 我 床 ， 月 以 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 月 兮 露 。 月 兮 月 兮 月 兮 河 汉 河 汉 月 兮 河 汉 水 流 天 河 汉 之 星 河 汉 之 月 兮 河 汉 兮 河 山 河 汉 之 ， 河 汉 之 云 。 河 汉 兮 河 汉 之 露 兮 河 汉 之 。 天 河'}, {'generated_text': '[CLS] 万 叠 春 山 积 雨 晴 ， 山 看 不 了 。 不 知 天 气 暄 ， 但 觉 日 光 好 。 山 人 睡 足 时 ， 心 闲 境 自 少 。 哉 山 中 人 ， 何 年 丹 九 转 。 毋 五 岳 游 ， 不 必 身 插 羽 。 客 方 兴 云 ， 云 归 自 何 许 。 乎 勿 叹 留 ， 不 归 待 渠 补 。 夜 大 雷 雨 ， 一 山 多 雨 声 。 檐 溜 响 不 歇 ， 端 似 秋 不 平 。 山 前 溪 水 流 ， 江 水 日 夜 响 。 江 湖 浮 画 舷 为 簸 ， 一 叶 荡 。 人 家 门 前 。 船 上 。 船 船 行 船 ， 水 去 ， 到 底 住 ， 一 掉 一 掷 一 掷 一 掷 一 掷 一 掷 一 掷 双 桨 ， 一 掷 一 樯 ， 不 摇 。 一 掷 一 掷 。 一 掷 一 掷 千 。 百 万 千 千 呼 。 一 掷 一 掷 一 掷 一 掷 一 掷 一 掷 一 掷 一 掷 一 掷 千 寻 一 换 百 舟 一 掷 一 掷 斗 一 掷 一 赌 一 掷 。 长 一 赌 一 掷 。 船 ， 一 赌 一 掷 一 赌 百 。 何 用 一 赌 一 掉 一 赌 得 千 偿 一 ， 换 一 掷 。'}]\n"
     ]
    }
   ],
   "source": [
    "# 还可以通过左边的语言 tag 选择其他语言的模型。例如加载专门用于生成中文古诗的 gpt2-chinese-poem 模型：\n",
    "#pip install --upgrade torch torchvision torchaudio\n",
    "\n",
    "# \n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"uer/gpt2-chinese-poem\")\n",
    "results = generator(\n",
    "    \"[CLS] 万 叠 春 山 积 雨 晴 ，\",\n",
    "    max_length=40,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
